{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191a86c9-01f0-4da2-80c2-b7c96f3a0d66",
   "metadata": {},
   "source": [
    "**Demand Prediction Example**  \n",
    "\n",
    "alternative to sigmoid is just striaght line for positive values.  \n",
    "e.g. g(z) = max(0,z)\n",
    "\n",
    "This is known as ReLU  \n",
    " - Rectivied Linear Unit  \n",
    "\n",
    "Also Linear activation function:  \n",
    "g(z) = z; allows negative values  \n",
    "sometimes people say 'no activation function'  \n",
    "Would be talking about linear activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b05c7-57d5-4dce-afdd-7222b6d35c08",
   "metadata": {},
   "source": [
    "**How to choose activation function**  \n",
    "\n",
    "Binary classification: Sigmoid  \n",
    "\n",
    "Regression: y = +/-: Linear activation function \n",
    "\n",
    "Regression: y = 0/+: ReLU\n",
    "\n",
    "\n",
    "<u> What about hidden layers? <u/>  \n",
    "\n",
    "ReLU most common choice - use as default  \n",
    " - ReLU faster than sigmoid to compute\n",
    " - ReLU goes flat at one part of the graph\n",
    " - If you have a function that is flat in a lot of places, gradient descent will be slow  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90818687-89af-403e-beb5-cd089b306b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      3\u001b[0m     Dense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, acitvation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     Dense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     Dense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Sequential\n",
    "model = Sequential([\n",
    "    Dense(units = 25, acitvation = 'relu'),\n",
    "    Dense(units = 15, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59921bcd-5831-4dac-821b-a687f38e6cd6",
   "metadata": {},
   "source": [
    "**Why do we need activation functions?**  \n",
    "\n",
    " If all functions are linear, it would be no different to linear regression  \n",
    "\n",
    " <u> Linear example: <u/>  \n",
    "\n",
    "One feature -> one hidden unit w1, b1; outputs a[1] -> output layer w2,b2; outputs a[2]\n",
    "\n",
    "If we just use g(z) = z  \n",
    "\n",
    "$a^{[1]} = w_{1}^{[1]}x + b_{1}^{[1]}$\n",
    "\n",
    "$a^{[2]} = w_{1}^{[2]}a^{[1]} + b_{1}^{[2]}$\n",
    "\n",
    "sub in. a2 is just some linear function of the input x. can just use linear regression model for that.  \n",
    "A linear function of a linear function is a linear function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288e79b-37ce-4086-a94f-f3235983edf2",
   "metadata": {},
   "source": [
    "**Examples**  \n",
    "\n",
    "Hidden layers all linear + output linear:  \n",
    " - Equivalent to linear regression\n",
    "\n",
    "Hidden layers linear but output is sigmoid:  \n",
    " - Equivalent to logistic regression\n",
    "\n",
    "So common rule of thumb is to not use linear activation functions in hidden layers - use ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f43521-20b8-4f03-bc18-ce786d340919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
