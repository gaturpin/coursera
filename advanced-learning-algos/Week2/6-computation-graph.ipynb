{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b329834-c38a-4ec3-8ab2-652dec62f974",
   "metadata": {},
   "source": [
    "**Computation Graph**  \n",
    "\n",
    "Start with small neural network example:  \n",
    "- x -> layer 1, 1 neuron -> a, output\n",
    "- w = 2, b = 8, x = -2, y = 2\n",
    "- a = wx + b\n",
    "- linear activation a = g(z) = z\n",
    "- J(w,b) = 0.5(a-y)^2\n",
    "\n",
    "<u> Forward prop - compute J<u/>  \n",
    "\n",
    "w = 2 -> c = wx = -4 -> a = c+b = 4 -> d = a-y = 2 -> J = 0.5*d^2 = 2  \n",
    " \n",
    "this is a computation graph - set of notes connected by arrows - check video for drawing of it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322581ab-eb9d-417f-b0d3-413e4b53b012",
   "metadata": {},
   "source": [
    "<Backward prop - Compute derivatives <u/>  \n",
    "\n",
    "This goes right to left / backwards  \n",
    "\n",
    " - If d changes a little how does J change - derivative is 2\n",
    " - If a changes a little how does a change - derivative of J w.r.t a is 2\n",
    "\n",
    "Chain rule to caculate derivatives w.r.t J at each step  \n",
    "\n",
    "Continue calculating dJ/dc, dJ/db, dJ/dw\n",
    "\n",
    "Each step is just a partial derivative of J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c08ae-4b9f-4564-8cde-63e6be6e412f",
   "metadata": {},
   "source": [
    "Back prop is an efficient way to compute derivatives  \n",
    "\n",
    "We go backwards because if we ask dJ/dw first we need to know dJ/dc etc...  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
